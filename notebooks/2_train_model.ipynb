{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# from pytorch_lightning import Trainer\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch_geometric.nn import global_mean_pool\n",
    "# from torchvision import transforms\n",
    "\n",
    "from layout_gnn.dataset.dataset import RICOTripletsDataset, DATA_PATH, RICOSemanticAnnotationsDataset\n",
    "from layout_gnn.dataset.transforms.core import process_data, normalize_bboxes\n",
    "from layout_gnn.dataset.transforms.pyg import convert_graph_to_pyg\n",
    "from layout_gnn.dataset.transforms.image import RescaleImage\n",
    "from layout_gnn.dataset.transforms.nx import add_networkx, ConvertLabelsToIndexes\n",
    "# from layout_gnn.nn.model import LayoutGraphModel\n",
    "# from layout_gnn.nn.neural_rasterizer import CNNNeuralRasterizer\n",
    "# from layout_gnn.lightning_module import LayoutGraphModelCNNNeuralRasterizer\n",
    "# from layout_gnn.utils import pyg_triplets_data_collate\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from functools import partial\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = partial(ImageClassification, crop_size=64, resize_size=(64, 64))()\n",
    "data = RICOSemanticAnnotationsDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(transforms(data[0]['image']).unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'root_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m RICOTripletsDataset(triplets\u001b[39m=\u001b[39;49mDATA_PATH \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpairs_0_10000.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m label_mappings \u001b[39m=\u001b[39m {k: i \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset\u001b[39m.\u001b[39mlabel_color_map)}\n\u001b[1;32m      3\u001b[0m dataset\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m     process_data,\n\u001b[1;32m      5\u001b[0m     normalize_bboxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     convert_graph_to_pyg,\n\u001b[1;32m     13\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'root_dir'"
     ]
    }
   ],
   "source": [
    "dataset = RICOTripletsDataset(triplets=DATA_PATH / \"pairs_0_10000.json\")\n",
    "label_mappings = {k: i for i, k in enumerate(dataset.label_color_map)}\n",
    "dataset.transform = transform=transforms.Compose([\n",
    "    process_data,\n",
    "    normalize_bboxes,\n",
    "    add_networkx,\n",
    "    RescaleImage(256, 256, allow_missing_image=True),\n",
    "    ConvertLabelsToIndexes(\n",
    "        node_label_mappings=label_mappings,\n",
    "        # edge_label_mappings={\"parent_of\": 0, \"child_of\": 1},\n",
    "    ),\n",
    "    convert_graph_to_pyg,\n",
    "])\n",
    "\n",
    "# data_loader = DataLoader(dataset=dataset, batch_size=8, collate_fn=pyg_triplets_data_collate)\n",
    "# model = LayoutGraphModelCNNNeuralRasterizer(\n",
    "#     num_labels=len(label_mappings) + 1, \n",
    "#     cnn_output_dim=3,\n",
    "#     cnn_output_size=256,\n",
    "# )\n",
    "# trainer = Trainer(default_root_dir=DATA_PATH)\n",
    "# model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drz/Layout-GNN/.venv/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:107: UserWarning: attribute 'readout' removed from hparams because it cannot be pickled\n",
      "  rank_zero_warn(f\"attribute '{k}' removed from hparams because it cannot be pickled\")\n",
      "/home/drz/Layout-GNN/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name                | Type                          | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | encoder             | LayoutGraphModel              | 42.3 K\n",
      "1 | decoder             | CNNNeuralRasterizer           | 1.2 M \n",
      "2 | triplet_loss        | TripletMarginWithDistanceLoss | 0     \n",
      "3 | reconstruction_loss | MSELoss                       | 0     \n",
      "----------------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.066     Total estimated model params size (MB)\n",
      "/home/drz/Layout-GNN/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067d37d7bd204712b5d9649524e6bf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drz/Layout-GNN/.venv/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58ee78e75df2536acc948db9544a0358bbd2b7577cf9c070409e62165bffe4bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
